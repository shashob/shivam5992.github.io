{'text' : """<p>In any data analysis, data preparation accounts for the majority of the effort, 70% of the time is involved in curating and fixing the dataset. It is necessary to get acquainted with each and every dimension of the available data before any analysis, Data Munging is the crucial component of data science which involves all the activities of exploring, tweaking and customizing the dataset according to the problem statement... <br><br> Read the complete article on mUniversity Blog official website, <a href="http://muniversity.mobi/blog/getting-started-with-data-science-data-munging/" target="_blank">Here</a> <br /><a href="http://muniversity.mobi/blog/getting-started-with-data-science-data-munging/" target="_blank"><div class="img-post"><img src="http://muniversity.mobi/blog/wp-content/uploads/2015/12/798x398xprocess1-798x398.jpg.pagespeed.ic._ilshEFseU.jpg" /></div></a></p>""",'title' : 'Getting Started with Data Science - Data Munging','date' : 'Dec 30, 2015','desc' : 'Data Munging is the crucial component of data science which involves all the activities of exploring, tweaking and customizing the dataset according to the problem statement.','url' : 'blog/data-munging/'}
{'text' : """<p>Data Mining is the technique of creating a raw data set by capturing data from a data source. The term data mining though has a broader meaning when talked about analytics, but in this blog we will discuss about data mining as the first and initial step of any data science application which deals primarily with data collection and data extraction ... <br><br> Read the complete article on mUniversity Blog official website, <a href="http://muniversity.mobi/blog/getting-started-with-data-science-data-mining/" target="_blank">Here</a> <br /><a href="http://muniversity.mobi/blog/getting-started-with-data-science-data-mining/" target="_blank"><div class="img-post"><img src="http://muniversity.mobi/blog/wp-content/uploads/2015/12/798x398xprocess1-798x398.jpg.pagespeed.ic._ilshEFseU.jpg" /></div></a></p>""",'title' : 'Getting Started with Data Science - Data Mining','date' : 'Dec 10, 2015','desc' : 'Data Mining is a process of collecting data, extraction of data and preparation of raw data set. It results in formation of a datasets which are in the ready to analyse formats.','url' : 'blog/data-mining/'}
{'url': u'blog/neural_network_1/', 'date': u' 25 Nov, 2015', 'desc': u'This blog series will be series of notes about neural networks. Lets get started with first section about Perceptrons, Sigmoid neurons, gradient descent and neural network architecture.  ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        This blog series will be series of notes about neural networks. Let's get started with first section about Perceptrons, Sigmoid neurons, gradient descent and neural network architecture.                    </p><p>                        Handwritten Digit Recognition is one of the classic problem of machine learning. It is easy for humans but too complicated for a computer model. But, Neural Networks are a good tool to perform this task with good accuracy.                     </p><p><b>Perceptrons</b> - It is proven that any logical functions can be derived using NAND gates, in the similar manner, Perceptrons are the universal blocks of any computation models. Perceptrons, sometimes also called activation functions or the neurons which takes several binary units as input and produces single binary output.                     </p><div class="img-post"><img src="images/1.png" /></div><p>                        In perceptron, all input variables may not be significant to give an output, Hence a weight is assigned to each of them. These weights express the importance of each input with respect to output. Also, a threshold is chosen indicating about the output of perceptron.                     </p><p>                        The output of a perceptron can be given as weighted sum of inputs. The output equals 1 if the weighted sum of inputs is greater than a threshold value, 0 if it is  lesser than a threshold value. Representing the threshold as negative value of a bias term, gives us the following equation about the perceptron function.                    </p><div class="img-post"><img src="images/2.png" /></div><p>                        This gives an idea that there are two basic parameters of a perceptron function - weight and bias. Weight signifies importance of each input, while bias signifies how easy or difficult it becomes for model to give output value 1 or 0. Higher values of bias will always gives value 1, Lower values of bias will always gives value 0. These values are tuned using learning algorithms such as Stochastic Gradient Descent.                     </p><p><b> Sigmoid Neurons</b> - Since perceptrons accept and produce only binary values. Making small delta changes in inputs will not affect network's output. Sigmoid Neurons are introduced due to the intuition that a delta change in weights and bias creates delta change in the output. This is also the basic idea about Learning - make numerous delta changes in input parameters to make delta changes in output, keep decreasing cost function by delta changes, until cost function is minimized.                  </p><p>                    Sigmoid neurons takes input between 0 and 1, gives output between 0 and 1. In case of sigmoid neurons, activation function is f(wâ‹…x+b) : sigmoid function. Sigmoid neurons are derived using sigmoid function:                 </p><div class="img-post"><img src="images/3.png" /></div><p><b>Neural Network Architecture </b></p><div class="img-post"><img src="images/4.png" /></div><p>                    In the network shown above, there are three layers - input layer, hidden layer and output layer. The idea of the network is input to one layer is the output of previous layer, this type of Neural Networks are called Feed Forward Neural Networks. Design of hidden layers is'nt straightforward but follows heuristic design patterns.                 </p><p><b>Gradient Descent</b> - In MNIST training dataset, there are (x, y) elements, x represents a 784 row vector (28 * 28 ie 784 gray scale image) and y is a ten row vector indicating output. A feed forward neural network is created with this dataset. In order to train the model, we will use Stochastic Gradient Descent to get values of weights and bias parameters. Using Sigmoid Function, we make delta change in input, this creates delta change in output.                 </p><div class="img-post"><img src="images/5.png" /></div><p>We also define the cost function, also called MSE - Mean Squared Error</p><div class="img-post"><img src="images/6.png" /></div><p>                    This function is always non negative and it measures the current error in hypothesis function. Basically, we need to find the weights and bias that will make this cost function minimum. If the function C(v) is dependent upon input parameters v = [v1, v2]. To compute gradient descent, start taking small steps in direction of v1 and v2 until you reach the minimum.                     <br />                    Change in CF is the derivative of CF function with respect to its parameters                 </p><div class="img-post"><img src="images/7.png" /></div><p>                    A new parameter is chosen "eta" also called learning rate parameter. If change in CF is delta C, then change in parameters is</p><div class="img-post"><img src="images/8.png" /></div><br /><div class="img-post"><img src="images/9.png" /></div><p>To make learning process faster, Stochastic gradient descent chooses small mini batches of training inputs.</p><p><b>Neural Network in Action</b></p><p>Though the full code and data implementation is provided at the end, here is a breakdown of how to implement neural network in python. </p><span>!!!!-# Initialize weights, bias, layers and units                        def __init__(self,  sizes):    !!!!self.num_layers = len(sizes)    !!!!self.sizes = sizes    !!!!self.biases = [np.random.randn(y, 1) for y in sizes[1:]]    !!!!self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]!!!!-!!!!# Given the input, calculates output of the network                        !!!!def feedforward(self, a):    !!!!for b, w in zip(self.biases, self.weights):        !!!!!!!!a = sigmoid(np.dot(w, a)+b)    !!!!return a!!!!-# Learning Algo: Gradient Descent                        def SGD(self, training_data, epochs, batch_size, eta,test_data=None):    !!!!for j in xrange(epochs):        !!!!!!!!random.shuffle(training_data)    !!!!!!!!mini_batches = [training_data[ k: k + batch_size ] for k in xrange(0, n, batch_size)]    !!!!!!!!for mini_batch in mini_batches:        !!!!!!!!!!!!self.update_mini_batch(mini_batch, eta)!!!!-# Update Mini Batch, Tune the weights, Decrease Cost Functiondef update_mini_batch(self, mini_batch, eta):       !!!!nabla_b =  [ np.zeros( b.shape ) for b in self.biases ]    !!!!nabla_w = [ np.zeros( w.shape ) for w in self.weights ]    !!!!for x, y in mini_batch:        !!!!!!!!delta_nabla_b, delta_nabla_w = self.backprop(x, y)        !!!!!!!!nabla_b = [ nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]        !!!!!!!!nabla_w = [ nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]    !!!!!!!!self.weights = [ w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]    !!!!!!!!self.biases = [ b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]!!!!-# function to calculate sigmoiddef sigmoid(z):    !!!!return 1.0/(1.0+np.exp(-z))                    </span><p><a href="https://github.com/shivam5992/neural-networks-notes">Here</a> is the link or complete code and data                        Stay Tuned, More notes coming.                    </p><br /""", 'title': u'Notes: Neural Network 1'}
{'url': u'blog/datascience-importance/', 'date': u' 23 Nov, 2015', 'desc': u'Data is everywhere around us, More than 200 million users on twitter, share content, videos and photos every day (Social Media Data). 500 Million smartphones are used for making calls, texts messaging every second (Smartphone User Data, Calls Data and Message Text Data)...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        Data is everywhere around us - More than 200 million users on twitter, share content, videos and photos every day (Social Media Data). 500 Million smartphones are used for making calls, texts messaging every second (Smartphone User Data, Calls Data and Message Text Data). 10 thousand airplanes are always off the ground in the sky (Aviation Data). More than 10 Million bank transactions are carried out daily (Bank Data) ...             <br /><br />                        Read the complete article on mUniversity Blog official website, <a href="http://muniversity.mobi/blog/understanding-the-growing-importance-and-meaning-of-data-science/" target="_blank">Here</a> <br /><a href="http://muniversity.mobi/blog/understanding-the-growing-importance-and-meaning-of-data-science/" target="_blank"><div class="img-post"><img src="http://muniversity.mobi/blog/wp-content/uploads/2015/11/798x319xdatascienceimp-798x319.jpg.pagespeed.ic.fKNsLMyh_y.jpg" /></div></a></""", 'title': 'Understanding the growing importance and meaning of data science'}
{'url': u'blog/social-data/', 'date': u' 17 Nov, 2015', 'desc': u'Social media is a huge collection of communication streams comprised of large scale user inputs, user to user interactions, content sharing and collaboration. These channels engage millions of people across the globe, who share content in the form of tweets, posts, videos and photos, thus generating billions of rows of data every second. ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        Social media is a huge collection of communication streams comprised of large scale user inputs, user to user interactions, content sharing and collaboration. These channels engage millions of people across the globe, who share content in the form of tweets, posts, videos and photos, thus generating billions of rows of data every second. Due to its vast dimensions, social media is gold mine of researchers, companies and organizations. Various types of data analysis such as Networks, Demographics, and Sentiments using machine learning, natural language processing and statistics, results in huge amount of fruitful information. This information is a value add for the decision makers. The insights extracted from this unstructured data can result in increased profits, improved revenues and increased sales...  <br /><br />    Read Complete Blog on Linkedin <a href="https://www.linkedin.com/pulse/social-media-data-analysis-its-industrial-impact-shivam-bansal?trk=prof-post/" target="_blank">Here</a> <br /><br /><a href="https://www.linkedin.com/pulse/social-media-data-analysis-its-industrial-impact-shivam-bansal?trk=prof-post" target="_blank"><div class="img-post"><img src="https://media.licdn.com/mpr/mpr/jc/AAEAAQAAAAAAAAbdAAAAJDU2YjIwOGYxLWQ1OWYtNDg0MC1iYjZmLTIxNWQ4MzM5ZjlmZA.png" /></div></a></""", 'title': u'Social media data analysis and its industrial impact'}
{'url': u'blog/text-classification-improvement/', 'date': u' 29 Oct, 2015', 'desc': u'A few months back, I was working on creating a sentiment classifier for Twitter data. After trying the common approaches, I was still struggling to get good accuracy  ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        A few months back, I was working on creating a sentiment classifier for Twitter data. After trying the common approaches, I was still struggling to get good accuracy on the results.                        Text classification problems and algorithms have been around for a while now. They are widely used for Email Spam Filtering by the likes of Google and Yahoo, for conducting sentiment analysis of twitter data and automatic news categorization in google alerts.                        <br /><br />                     Read Complete Blog on Analytics Vidhya <a href="http://www.analyticsvidhya.com/blog/2015/10/6-practices-enhance-performance-text-classification-model/" target="_blank">Here</a> <br /><br /><a href="http://www.analyticsvidhya.com/blog/2015/10/recommendation-engines/" target="_blank"><div class="img-post"><img src="http://i2.wp.com/www.analyticsvidhya.com/wp-content/uploads/2015/10/6-practices_1.jpg?resize=500%2C300" /></div></a></""", 'title': u'Practices to Improve Text Classification Model'}
{'url': u'blog/recommendation-engine/', 'date': u' 11 Oct, 2015', 'desc': u'Ever wondered, what algorithm google uses to maximize its target ads revenue?. What about the e-commerce websites which advocates you through options such as ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        Ever wondered, what algorithm google uses to maximize its target ads revenue ? What about the e-commerce websites which advocates you through options such as people who bought this also bought this. Or How does Facebook automatically suggest us to tag friends in pictures?                        The answer is Recommendation Engines. With the growing amount of information on world wide web and with significant rise number of users, it becomes increasingly important for companies to search, map and provide them with the relevant chunk of information according to their preferences and tastes.                        <br /><br /><br />                        Read Complete Blog on Analytics Vidhya <a href="http://www.analyticsvidhya.com/blog/2015/10/recommendation-engines/" target="_blank">Here</a> <br /><a href="http://www.analyticsvidhya.com/blog/2015/10/recommendation-engines/" target="_blank"><div class="img-post"><img src="http://www.analyticsvidhya.com/wp-content/uploads/2015/10/few-things2.jpg" /></div></a></""", 'title': u'Basics of Recommendation Engines'}
{'url': u'blog/text-cleaning-python/', 'date': u' 14 Nov, 2014', 'desc': u'The days when one would get data in tabulated spreadsheets are truly behind us. A moment of silence for the data residing in the spreadsheet pockets. Today, more than 80% of the data is unstructured ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        The days when one would get data in tabulated spreadsheets are truly behind us. A moment of silence for the data residing in the spreadsheet pockets. Today, more than 80% of the data is unstructured - it is either present in data silos or scattered around the digital archives. Data is being produced as we speak - from every conversation we make in the social media to every content generated from news sources. In order to produce any meaningful actionable insight from data, it is important to know how to work with it in its unstructured form. As a Data Scientist at one of the fastest growing Decision Sciences firm, my bread and butter comes from deriving meaningful insights from unstructured text information.                       <br /><br />                        Read Complete Blog on Analytics Vidhya <a href="http://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/" target="_blank">Here</a> <br /><br /><a href="http://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/" target="_blank"><div class="img-post"><img src="http://www.analyticsvidhya.com/blog/wp-content/uploads/2014/11/Mining_Twitter_Data.jpg" /></div></a></""", 'title': u'Steps for text data cleaning'}
{'url': u'blog/everything_you_need_to_know_about_text_parsing/', 'date': u' Aug 22, 2014', 'desc': u'Parsing is the process of analysing an object consisting of various datapoints according to either rules of a formal grammar or syntatical data patterns or both ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p><b>What is text parsing ?</b><br />                    Parsing is the process of analysing an object consisting of various datapoints according to either rules of a formal grammar or syntatical data patterns or both. Parsing is defined as capturing the valuable points, encountered while traversing a data. When data is in textual form, the term is refered as text parsing. It's a common task of Natural Language Processing and is widely used nowdays in various computer specific domains.   <br /><br /><b>Why there is a need of text parsing ?</b><br />                    Textual data is growing rapidly. It involves billions of social media conversations, news, customer reviews, product descriptions, companies annual reports, financial reports, intellectual properties, speech transcripts, user search and many more. Most of this data is highly unstructured and non analysable formats. One cannot simply play with this data for research purposes, decision making, analysis or predicting the outcomes. It is text Parsing that gives information and insights from this data, which ultimately creates knowledge and wisdom. So, text parsing means converting unstructured data into structured, analysable, playable formats.                    <br /><br /><b>Approaches for text parsing ?</b><br />                    Text parsing is generally performed using two approaches. One is Data driven approach, other is grammer driven. For better accuracy purposes, a mix of both is required. Text Parsing is performed in hierarichial manner which starts with removal of unuseful context from the data, the next step is to perform analysis on the left over chunks using number of techniques. <br /><br /><b>Techniques used for text parsing ?</b><br /></p><p><b>Lexicon Level</b><br /><li>Lexicon lookup table - Most widely considered method is to create a domain specific data warehouse of lexicons. While traversing text, each token is looked up into database and results are created accordingly. </li><li>Word matching - It involves various methods like fuzzy string matchng, elasticsearch, levenshtein distances, naive string matching algos like KNP, rabin karp.</li><li>Key value pairs - Similar to look ups but there is an additional value correspoding to keys(lexicons)</li><li>Stemming - Sometimes exact words are not helpful but their lemmas are, so words are stemmed to rool level.</li><li>Normalization - invloves both stemming, lemmatization and converting processed words into proper form</li></p><p><b>Context level</b><br /><li>Combinations of words: not single word but their combinations in the sentences are considered, N grams are helpful and are given more priority.</li><li>Regex - widely used to find capture insights using patterns for example date matching, number caprturing, particular formats in the text.</li><li>Case Checking some rules can be applied by considering upper cases capitalization.</li><li>Type Checking alphanumberic, digits etc</li><li>Wordlength - sometimes wordlength are helpful</li></p><p><b>Grammer level</b><br /><li>Pos tags - Beauty of NLP, part of speech tags like verbs, nouns , adjectives helps to identify insights.</li><li>Tenses - present future past tenses are also used for rules creation.</li><li>Context free grammetical relations - negations, intensifiers, modifiers, nsubjects, complemetries etc.</li></p><p><b>Higher level NLP</b><br /><li>NER - named entities like location, person name etc.</li><li>Dependency parse trees - parsing trees based on grammer rules with syntatcial rules</li></p><p>Share your views in the comments if some points are missing and do share your experiences with text parsing.</p>""", 'title': u'Everything you need to know about text parsing'}
{'url': u'blog/operational_transform/', 'date': u' Aug 09, 2014', 'desc': u'When multiple sites are collaborating on a single text document, Order of events occured at thses sites is important ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>  In a recent hackathon at my workplace, I wanted to create a text editor with real time collaborative editing feature similar to that in Google Docs. While researching for this functionality, I came to know about the algorithm behind it known as Operational Trasform. According to Wikipedia, OT is invented for consistency maintenance and concurrency control in collaborative editing of plain text documents.  <br /><br /><b>Operational Transform:</b> When multiple sites are collaborating on a single text document, Order of events occured at thses sites is important. It the the role of OT to maintain the order and execute them. OT supports concurrency by the use of consistency models. These models have features like Causality (Ensuring execution of events in their order), Convergence ( All occured events have been executed at each site) and Intention (Reflecting effects on each site).                        <br /><br /><a href="http://www.firepad.io/#1">Firepad</a> An open source collaborative text editor is designed to add the real time functionality to dynamic websites. It is by deployed on <a href="https://www.firebase.com/">Firebase</a> app platform and  provides true collaborative editing, complete with intelligent operational transform-based merging and conflict resolution.                        <br /></p><p>OT is an awesome algorithm for adding some cool functionalities of modern day web applications. For more knowledge, refer to following links and references .                        <br />                        Resources and links:                        <ul align="left"><li><a href="http://www.firepad.io/#1">Firepad</a></li><li><a href="http://sharejs.org/">ShareJs</a></li><li><a href="https://github.com/Operational-Transformation/ot.js/">ot.js</a></li><li><a href="https://togetherjs.com/source/">TogetherJs</a></li><li><a href="http://www.codecommit.com/blog/java/understanding-and-applying-operational-transformation"> Understanding and applying operational transform</a></li><li><a href="http://en.wikipedia.org/wiki/Operational_transformation">Wikipedia: Operational Transform</a></li><li><a href="http://www.waveprotocol.org/whitepapers/operational-transform">Google Wave Operational Transformation</a></li><li><a href="http://dl.acm.org/citation.cfm?doid=215585.215706">High-latency, low-bandwidth windowing in the Jupiter collaboration system</a></li><ul></ul></ul></""", 'title': u'Operational Transform'}
{'url': u'blog/epoc_emotiv/', 'date': u' May 01, 2014', 'desc': u"Ever imagined, Controlling the world around with the help of brain, thoughts and emotions. Imagine if, someone's feelings producing an real time action like automated facebook chatting ... <div class='img-post'><img src='img/epoc.jpg'></div>", 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                    Ever imagined, Controlling the world around with the help of brain, thoughts and emotions. Imagine if, someone feelings producing an real time action like automated facebook chatting, changing the gears of a car or drawing in paint application without using hands. Well, <a href="http://emotiv.com/" target="_blank">Epoc Emotiv</a> has made it possible.                     <br /><br />                    The Australian based company <i>emotiv</i> with more than 10 years research has made a killer product named "EPOC EMOTIV". Its basically a kind of headset with a number of sensors attached to it, The role of the sensors is to extract the brain wave patterns or signals from human brain. Emotiv has presented a revolutionary personal interface for interaction between humans and computers. Emotiv EPOC is a high resolution, multi-channel, wireless neuroheadset. The EPOC uses a set of 14 sensors plus 2 references to tune into electric signals produced by the brain to detect the user's thoughts, feelings and expressions in real time.                    <br /><div align="center"><div class="img-post"><img src="../../img/me_with_epoc.png" target="_blank" /></div><br /><br />Epoc Emotiv Headset</div></p><p>                    There are three suites included in epoc emotiv sdk namely affectiv, cognitive and expressive. Cognitive suite is the core feature of this product as it captures human thoughts. Epoc also supports motor actions, gyro actions and expressiv actions. Epoc is even capable to decide the user's mood, head movements, face expression, face actions and much more.                     <br /><br />                    When all the features are combined than a useful amount of data can be collected. This data can be smartly used by the researchers, developers of just the the user himself to create something useful. The device's output from human brain can be manipulated by the intelligent developers to create some unthinkable applications and products.                     <br /><br />                    Some of the major projects featuring epoc emotiv are:                    <ul align="left"><li>Controlling a 4 wheeler vehicle using your brain. Several actions are mapped to create different actions. </li><li>Robotic arm which is controlled by brain, playing gaming interfaces in mobiles and computers. </li><li>Authentication and identification using pass thoughts rather passwords. </li><li>Online drawing patterns in paint using brain. </li></ul></p><p>                    ME and my Friends <a href="http://www.mayankbhola.in" target="_blank">Mayank Bhola</a> and <a href="http://www.varunmalhotra.info" target="_info">Varun Malhotra</a> played with Epoc Emotiv for around 6 months and successfully implemented following:<br /><ul align="left"><li>Tic-Tac-Toe Gyro game</li><li>Tic-Tac-Toe Cognitive game</li><li>Pattern Authentication</li><li>Mind Motion State</li><li>Brain Wave Authentication</li><li>User Action Mappring to Computer Actions</li></ul></p><p><a href="https://www.flickr.com/photos/123442994@N03/sets/72157644050087837/">Here</a> are some of our memories with epoc emotiv. Feel free to contact, discuss and share !</p>""", 'title': u'Playing with Epoc Emotiv'}
{'url': u'blog/web_scraping_in_python/', 'date': u' Jan 01, 2014', 'desc': u'Web Scraping is a process of extracting out useful information form any website. Sometimes it is also refered as web mining, web crawling and web parsing ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        Web Scraping is a process of extracting out useful information form any website. Sometimes it is also refered as web mining, web crawling and web parsing, but all of them have the same meaning.                        Python provides many different modules and liberaries to achieve the purpose. Using techniques and proper modules of web scraping one can extract out useful information from the html text.                        Modules like urllib, requests etc are used to get the complete html text of a webpage from which data can be extracted.                         Beautiful Soup is a python liberary which is used for the purpose of web data mining. The scraping process starts with requesting a webpage from python script,                        parsing the reponse to html text followed by making Beautiful Soup object using html text and finally applying techniques to get useful data.                         The following tutorial will guide you to scrap a webpage.                        <br /><br /><b>Modules required:</b> urrlib(or requests, mechanize etc) and BeautifulSoup.                         <br />                        To Install these modules on your machine run the following scripts on your machine.                    </p><blockquote>                    pip install BeautifulSpup <br />                    pip install urllib                    </blockquote><p><b>Setup:</b> First, import the required modules</p><blockquote>                        from BeautifulSoup import BeautifulSoup<br />                        import urllib                    </blockquote><p><b>HTML Response:</b> The next step is to send a request to a webpage and get the response in the form of html text. For this purpose i have used urllib. The basic snippet is like this                     </p><blockquote>                    url = "http://www.google.com" <br />                    htmlfile = urllib.urlopen(url) <br />                    htmltext = htmlfile.read()                                </blockquote><p><b>The soup:</b> Using html text, we need to create an object of Beautiful Soup, the soup variable is now ready to apply methods and functions to get data.</p><blockquote>                    soup = BeautifulSoup(htmltext)                     </blockquote><p>Once soup object is initialied, we have got access to various methods and sytaxes to retrieve the data. Here is a small list:<br />                    Selecting by Tags:                    <blockquote>                    soup.head <br />                    soup.head.title <br />                    soup.body.h2.a                    </blockquote></p><p>Selecting all particular tags:</p><blockquote>                    soup.findAll('p') <br />                    for anchor in soup.findAll('a'): <br />                    &nbsp;&nbsp;&nbsp;&nbsp;print anchor                    </blockquote><p>Selcting a particular tag with some attribute</p><blockquote>                    soup.find('div',attrs={"class":"classname"}) <br />                    soup.findAll('p',attrs={"id":"idname"})                    </blockquote><p>Parent, Children and Sibling</p><blockquote>                    for the following scenario: <br />                     &lt;head&gt;  <br />                        &lt;title&gt; titlename &lt;/title&gt; <br />                        &lt;a&gt;random&lt;/a&gt; <br />                    &lt;/head&gt; <br /> <br />                    soup.title.parent # head <br />                    soup.head.children # title <br />                    soup.title.next_sibling # a <br />                    soup.a.prev_sibling # title <br />                    soup.a.string # random <br /></blockquote><br /><p>Here is the full code to scrap all anchor tags in google.com </p><blockquote>                    import urllib <br />                    from BeautifulSoup import BeautifulSoup <br />                    url = "http://www.google.com"<br />                    htmlfile = urllib.urlopen(url)<br />                    htmltext = htmlfile.read()<br />                    soup = BeautifulSoup(htmltext)<br />                    print soup.head.title.string<br />                    for link in soup.findAll('a'):<br />                        &nbsp;&nbsp;&nbsp;&nbsp;print link<br /></blockquote><p>Check out my web scrapping series in python repository on github. i have scrapped webistes like google.finance, yahoo.finance, bloomberg, google.movies, horoscope websites,                    nytimes.com, irctc.gov.in, weather websites etc.                    <a href="https://github.com/shivam5992/PyScrapper" target="_blank">Here</a> is the link.                    Feel free to discuss and share.                    </p><br /""", 'title': u'Web Scraping in Python'}
{'url': u'blog/deploy_flask_on_heroku/', 'date': u' Oct 18, 2013', 'desc': u'Heroku is one of the most popular cloud hosting service. It can be used to deploy the applications of Python, Php, Ruby, Node.js and many more online ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                        Heroku is one of the most popular cloud hosting service. It can be used to deploy the applications of Python, Php, Ruby, Node.js and many more online. This blog will teach you how to deploy the Python web apps that uses Flask web framework on the Heroku cloud step by step. It requires the use of some tools and some modules along with basic knowledge of git commands. So here we go:                        <br /><br />                        Before procedding make sure that GIT is installed on your machine. If its not then you must setup and install it first. Create an account on <a href="http://www.heroku.com/" target="_blank">Heroku's official website</a>. It's free and quick. After the account setup, let's get some tools and modules on your local machine, download and install heroku toolbelt from <a href="https://toolbelt.heroku.com/" target="_blank">here</a>. This is the command line support for heroku. Some python modules are required which can be easily downloaded using python's setup-tools. Download the latest version of setup tools from <a href="https://pypi.python.org/pypi/setuptools" target="_blank"> here </a>. Also install pip from <a href="http://www.pip-installer.org/en/latest/installing.html" target="_blank">here</a>.                         <br /><br />                        After installation of setup tools you will see a folder named venv in the downloaded directory. CD to venv folder and activate the venv to launch virtual environment using entering this command in cmd:                    </p><blockquote>                        activate.bat                     </blockquote><p>                        This will prompt a venv shell which is nothing but a virtual environment. In Venv shell Cd to your app directory and add a procfile to it. It's a text file which contains information about your app.sample Procfile for flask app:                    </p><blockquote>                    web: gunicorn hello:app                    </blockquote><p>                    Also add requirements.txt file in your repository using this command.                    </p><blockquote>                    Pip freeze &gt; requirements.txt                    </blockquote><p>                    Now all prerequisites are completed and its time to git init in that folder.                     Add all your files for tracking and commit your files using.                    </p><blockquote>                    git init<br />                    git add .<br />                    git commit -m "your commit message"                    </blockquote><p>                    In the command prompt login into your heroku account using following commands. After successful authentication                    create a new app in the heroku app dashboard and finally push your app to cloud.<br /></p><blockquote>                    heroku login<br />                    heroku create<br />                     git push heroku master                    </blockquote><p>                    Make sure you have added ssh keys to heroku account, otherwise you will get error.<br />                    Trobuleshooting for errors related to ssh keys: <br /><br />                    First, Check for the existing ssh keys on your machine.                     In git bash cd to your .ssh folder (usually located in users/ in c drive).                    Check the folder  to see if you have a file named either id_rsa.pub or id_dsa.pub                     If not then generate a new SSH key, use the command                    </p><blockquote>                    Ssh-keygen-t rsa -c "youremal@example.com"                    </blockquote><p>                    Now you will be prompted to to enter a passphrase. Its just like a password.                    After key successfully generated you need to add them to heroku account, use following command                     </p><blockquote>                    Heroku keys:add                    </blockquote><p>                    I Hope that this blog will help you deploy your app on heroku cloud. Feel free to contact :)                    </""", 'title': u'Deploy Flask Application on Heroku'}
{'url': u'blog/my_android_experience/', 'date': u' Oct 12, 2013', 'desc': u'The intent of this blog is to give an outline of my academic android project. After scouting for different app ideas, I decided to make a utility application on the android platform ...', 'text': """<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 arttext"><p>                    The intent of this blog is to give an outline of my academic android project.                     <br /><br />                    After scouting for different app ideas, I decided to make a utility application on the android platform which was supposed to control the computers according to voice instructions given by the user on their mobile phones.                     <br /><br />                    I decided to name this app "APVC", acronym for Android Pc Voice Controller. This application was supposed to operate Pcs, used for quick and accurate text typing, playing audio and video files and a number of more functionalities.                    <br /><br />                    To get a comfort level in android platform, I created some sample applications in android like calculator, text editor and a mini music player. In case of APVC, networking, speech to text voice recognization were important features. For the networking purposes, I used WIFI communication channel to establish a connection between the server and the client ie. mobile and the PC. The app was able to pass textual data between the two.                     <br /><br />                    To Implement the voice recognition feature, I implemented the speech to text android intent in my application and it seemed very appalling. The app was able to receive the text version of whatever was spoken between the two platforms.                    <br /><br />                    I integrated all the distributed snippets to give a structure to complete project. Later the feature of authentication was added. Only if the correct key was passed, the app server would response. In the server side, multithreading was introduced to execute the functions and process synchronously. I created the methods that manipulated the received input strings. For example if text "Open Notepad" was received, then the methods would execute a command that would open the notepad in the pc.                     <br /> <br />                     In the similar manner, I created about 25 functionalities that would perform different tasks. I developed the feature of text typing in my project. The same functionality was replicated on the notepad, browser, chatting, word pad and even command prompt!, which indeed opened up space for many new operations like command line aspects like shutdown, system sleep, opening and closing applications. One interesting feature of this project was opening and playing audio and video files.  The user could explore any directory and play the present audio and video files. Later I added the functions to control sound and speed as well. Features like closing the opened window and entering into new line, special characters typing were also introduced.                    <br /><br />                                         That's all my about APVC, you can check out the project <a href="https://github.com/shivam5992/AndroidPcVoiceController">Here</a>.                    Feel free to discuss and comment.                    </p><br /""", 'title': u'Android Pc Voice Controller'}